{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\"\"\" El número de cores que vamos a utilizar la indicamos al crear el SparkContext mediante el argumento \n",
    "local[n] siendo n el número de cores.\n",
    "\n",
    "Para cambiar este número deberemos parar el SparkContext mediante sc.stop y volver a lanzarlo cambiando n\"\"\"\n",
    "sc = SparkContext(\"local[6]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definción de las funciones que usaremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de procesado de los RDDs al formato deseado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesado(rdd):\n",
    "    \"\"\" 1º Mapeamos para dividir cada línea en los distintos elementos usando como separador la coma.\n",
    "    \n",
    "    2º Mapeamos de manera que tenemos (usuario, modelo, actividad) como clave y luego tenemos las 3\n",
    "    posiciones en X,Y,Z. Las mapeo 3 veces de manera normal para el cálculo de la media, el máximo y el\n",
    "    mínimo y los mapeo una vez al cuadrado para sacar la std. El últimmo elemento del mape será el uno \n",
    "    que ya hemos usado varias veces para hacer un conteo de los elementos y poder calcular la media.\n",
    "    \n",
    "    3º Reducimos mediante la llave que hemos definido antes y hacemos las siguientes operaciones 2 a 2:\n",
    "        - Sumamos los elementos 0, 1, 2 para calcular luego la media\n",
    "        - Sumamos los elementos 3, 4, 5 para calcular luego las std\n",
    "        - Con los valores 6, 7, 8 nos vamos quedando con el valor máximo a cada par de datos\n",
    "        - Con los valores 9, 10, 11 nos vamos quedando con el valor mínimo a cada par de datos\n",
    "    \n",
    "    4º Mapeamos los valores (usamos mapValues por \"aplanar\" la salida) de manera que:\n",
    "        - Dividimos los valores 0, 1, 2 entre el 12 que tiene el número total de datos y tenemos la media\n",
    "        - Hacemos lo mismo con los valores cuadráticos 3, 4, 5 siendo un paso intermedio para la std\n",
    "        - Los valores 6 - 11 representan ya los máximos y mínimos en x,y,x respectivamente así que los\n",
    "        pasamos directamente sin cambiar\n",
    "        \n",
    "    5º Por último tenemos un map que nos da:\n",
    "        - Los valores 0 - 2 (media), 6 - 8 (máximo), 9 - 11 (mínimo) ya son resultados finales así que los\n",
    "        pasamos tal cual\n",
    "        - Los valores 3, 4, 5 (cuadrados entre el número total de datos) los restamos al valor de la media\n",
    "        al cuadrado y luego hacemos la raiz cuadrada como hicimos en los ejercicios básicos\n",
    "        \"\"\"\n",
    "    rdd_fin = (rdd\n",
    "                   .map(lambda l: l.split(','))\n",
    "                   .map(lambda p: [(p[6], p[7], p[9])\n",
    "                                   ,(float(p[3]), float(p[4]), float(p[5])\n",
    "                                     ,float(p[3])**2, float(p[4])**2, float(p[5])**2\n",
    "                                     ,float(p[3]), float(p[4]), float(p[5])\n",
    "                                     ,float(p[3]), float(p[4]), float(p[5]),1)])\n",
    "                   .reduceByKey(lambda v1,v2: (v1[0]+v2[0], v1[1]+v2[1], v1[2]+v2[2]\n",
    "                                              ,v1[3]+v2[3],v1[4]+v2[4], v1[5]+v2[5]\n",
    "                                              ,v1[6] if v1[6]>v2[6] else v2[6], v1[7] if v1[7]>v2[7] else v2[7], v1[8] if v1[8]>v2[8] else v2[8]\n",
    "                                              ,v1[9] if v1[9]<v2[9] else v2[9], v1[10] if v1[10]<v2[10] else v2[10], v1[11] if v1[11]<v2[11] else v2[11]\n",
    "                                              ,v1[12]+v2[12]))\n",
    "                   .mapValues(lambda v: (v[0]/v[12],v[1]/v[12],v[2]/v[12]\n",
    "                                         ,v[3]/v[12],v[4]/v[12],v[5]/v[12]\n",
    "                                         ,v[6],v[7],v[8]\n",
    "                                         ,v[9],v[10],v[11]))\n",
    "                   .mapValues (lambda v: (v[0],v[1],v[2]\n",
    "                                          ,(v[3]-v[0]**2)**0.5,(v[4]-v[1]**2)**0.5,(v[5]-v[2]**2)**0.5\n",
    "                                          ,v[6],v[7],v[8]\n",
    "                                          ,v[9],v[10],v[11])))\n",
    "    return rdd_fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de unión de los 4 RDDs a uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JoinAll(rdd1, rdd2, rdd3, rdd4):\n",
    "    \"\"\" \n",
    "    Los 4 argumentos que le pasamos son los 4 RDDs que estamos creando a partir de los ficheros.\n",
    "    \n",
    "    Unimos por un lado los RDDs de los teléfonos y por otro los de los relojes mediante un join \n",
    "    aprovechando que tienen las mismas claves 2 a 2.\n",
    "    \n",
    "    Finalmente unimos los 2 RDDs resultantes en otro. Estos no comparten claves así que tenemos que \n",
    "    usar otro tipo de join. Podríamos usar Left o Right indiferentemente. También podríamos usar Full,\n",
    "    que nos daría lo mismo pero con más Nones por las no coincidencias de las llaves.\n",
    "    \n",
    "    Otra opción sería usar union directamente como se dice en el enunciado debido a que tienen la misma\n",
    "    estructura los 4 RDDs y por lo tanto no nos dará ningún problema\"\"\"\n",
    "    \n",
    "    joined_1 = rdd1.join(rdd2) \n",
    "    joined_2 = rdd3.join(rdd4)\n",
    "    joined = joined_2.union(joined_1)\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de los ficheros para crear los RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_ph_ac = sc.textFile(\"Phones_accelerometer.csv\")\n",
    "lines_ph_gy = sc.textFile(\"Phones_gyroscope.csv\")\n",
    "lines_wa_ac = sc.textFile(\"Watch_accelerometer.csv\")\n",
    "lines_wa_gy = sc.textFile(\"Watch_gyroscope.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Aplicamos la primera función para transformar los cuatro RDDs con las líneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_ph_ac = procesado(lines_ph_ac)\n",
    "rdd_ph_gy = procesado(lines_ph_gy)\n",
    "rdd_wa_ac = procesado(lines_wa_ac)\n",
    "rdd_wa_gy = procesado(lines_wa_gy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Aplicamos la segunda función para juntar los 4 RDDs en uno solo que tenga todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_all = JoinAll(rdd_ph_ac, rdd_ph_gy, rdd_wa_ac, rdd_wa_gy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Veamos por ejemplo si funciona sacando los 3 primeros valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_all.take(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para medir el tiempo que tarda en generar todo el RDD usamos el siguiente comando. Podemos una línea cualquiera después para que no se nos muestre todo el RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 23s\n",
      "ya\n"
     ]
    }
   ],
   "source": [
    "%time rdd_all.collect()\n",
    "print(\"ya\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para parar el contexto de Spark actual usamos el siguiente comando. Después lo que tendríamos que hacer es iniciarlo de nuevo con el nuevo número de cores que queramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabla de tiempos de ejecución para el collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Número de cores | Tiempo de ejecución |\n",
    "| :- | :- \n",
    "| 2 | 7min 12s\n",
    "| 4 | 4min 24s\n",
    "| 6 | 3min 59s "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vemos claramente que conforme aumentamos el número de cores se reduce el tiempo de ejecución. Pero esta reducción no es lineal, si no más bien logarítmica de manera que cada vez necesitamos muchos más cores para conseguir una reducción efectiva de tiempo de ejecución"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
